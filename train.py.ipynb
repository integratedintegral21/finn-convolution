{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:10.542186Z",
     "start_time": "2024-07-27T15:21:10.539503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting setuptools==69.5.0\n",
      "  Downloading setuptools-69.5.0-py3-none-any.whl (893 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m893.7/893.7 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyscaffold 4.4 requires platformdirs<3,>=2, but you have platformdirs 4.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed setuptools-69.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools==69.5.0\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat, Int32Bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e030abee64f24243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:11.352482Z",
     "start_time": "2024-07-27T15:21:11.346618Z"
    }
   },
   "outputs": [],
   "source": [
    "WEIGHT_BIT_WIDTH = 8\n",
    "ACT_BIT_WIDTH = 3\n",
    "\n",
    "class QuantModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_0 = qnn.QuantConv2d(\n",
    "            1,\n",
    "            6,\n",
    "            kernel_size=3,\n",
    "            bias=False,\n",
    "            weight_bit_width=WEIGHT_BIT_WIDTH)\n",
    "        self.relu_0 = qnn.QuantReLU(bit_width=ACT_BIT_WIDTH)\n",
    "        self.conv_1 = qnn.QuantConv2d(6, 16, 6,\n",
    "                                      weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "                                      bias=False)\n",
    "        self.relu_1 = qnn.QuantReLU(bit_width=ACT_BIT_WIDTH)\n",
    "        self.conv_2 = qnn.QuantConv2d(16, 128, 4,\n",
    "                                      weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "                                      bias=False)\n",
    "        self.fc1 = qnn.QuantLinear(128, 84,\n",
    "                                   weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "                                   bias=True)\n",
    "        self.relu_2 = qnn.QuantReLU(bit_width=ACT_BIT_WIDTH)\n",
    "        self.fc2 = qnn.QuantLinear(84, 10,\n",
    "                                   weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "                                   bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_0(x)\n",
    "        x = self.relu_0(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2, 2)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2, 2)\n",
    "        x = self.conv_2(x)\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f1fb149c27aef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:12.137494Z",
     "start_time": "2024-07-27T15:21:12.134802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895c6d91687a802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:13.396027Z",
     "start_time": "2024-07-27T15:21:13.392950Z"
    }
   },
   "outputs": [],
   "source": [
    "input_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88205ae5d68494ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:14.254806Z",
     "start_time": "2024-07-27T15:21:14.156238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  60000\n",
      "Val dataset size:  10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST('./data', train=True, download=True, transform=input_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.FashionMNIST('./data', train=False, download=True, transform=input_transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "print(\"Train dataset size: \", len(train_dataset))\n",
    "print(\"Val dataset size: \", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8075071867491929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:15.114310Z",
     "start_time": "2024-07-27T15:21:15.086475Z"
    }
   },
   "outputs": [],
   "source": [
    "model = QuantModel().to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f17c9e4cc6dd0bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:26:26.300125Z",
     "start_time": "2024-07-27T15:21:16.385219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Train loss: 2.324441010951996\n",
      "Batch 200: Train loss: 2.2975353479385374\n",
      "Batch 300: Train loss: 2.2933875203132628\n",
      "Batch 400: Train loss: 2.286476309299469\n",
      "Batch 500: Train loss: 2.2744906044006346\n",
      "Batch 600: Train loss: 2.254709367752075\n",
      "Batch 700: Train loss: 2.2172139477729798\n",
      "Batch 800: Train loss: 2.146773738861084\n",
      "Batch 900: Train loss: 2.036548342704773\n",
      "Val loss: 1.8961950016629165\n",
      "Epoch 1\n",
      "Batch 100: Train loss: 1.8155921840667724\n",
      "Batch 200: Train loss: 1.5335234880447388\n",
      "Batch 300: Train loss: 1.305894548892975\n",
      "Batch 400: Train loss: 1.1300686198472976\n",
      "Batch 500: Train loss: 1.0157148694992066\n",
      "Batch 600: Train loss: 0.9282804226875305\n",
      "Batch 700: Train loss: 0.8585860252380371\n",
      "Batch 800: Train loss: 0.8205051797628403\n",
      "Batch 900: Train loss: 0.7829470527172089\n",
      "Val loss: 0.7703432907724077\n",
      "Epoch 2\n",
      "Batch 100: Train loss: 0.7255081808567048\n",
      "Batch 200: Train loss: 0.725473797917366\n",
      "Batch 300: Train loss: 0.6851366755366325\n",
      "Batch 400: Train loss: 0.680619804263115\n",
      "Batch 500: Train loss: 0.6747423747181892\n",
      "Batch 600: Train loss: 0.6547303685545921\n",
      "Batch 700: Train loss: 0.6313828530907631\n",
      "Batch 800: Train loss: 0.6402478617429733\n",
      "Batch 900: Train loss: 0.6211467319726944\n",
      "Val loss: 0.6294033142032137\n",
      "Epoch 3\n",
      "Batch 100: Train loss: 0.602875452041626\n",
      "Batch 200: Train loss: 0.5948975756764412\n",
      "Batch 300: Train loss: 0.5911104795336724\n",
      "Batch 400: Train loss: 0.577762716114521\n",
      "Batch 500: Train loss: 0.5861758264899254\n",
      "Batch 600: Train loss: 0.5642371159791947\n",
      "Batch 700: Train loss: 0.5502753242850303\n",
      "Batch 800: Train loss: 0.5528554967045785\n",
      "Batch 900: Train loss: 0.5439777666330338\n",
      "Val loss: 0.5627477789760396\n",
      "Epoch 4\n",
      "Batch 100: Train loss: 0.5489468225836753\n",
      "Batch 200: Train loss: 0.5302812719345092\n",
      "Batch 300: Train loss: 0.5216717529296875\n",
      "Batch 400: Train loss: 0.5110641232132912\n",
      "Batch 500: Train loss: 0.510593732893467\n",
      "Batch 600: Train loss: 0.5156460157036782\n",
      "Batch 700: Train loss: 0.5084344920516014\n",
      "Batch 800: Train loss: 0.488163378238678\n",
      "Batch 900: Train loss: 0.4927342450618744\n",
      "Val loss: 0.513908857183092\n",
      "Epoch 5\n",
      "Batch 100: Train loss: 0.4815185505151749\n",
      "Batch 200: Train loss: 0.4949795126914978\n",
      "Batch 300: Train loss: 0.474196303486824\n",
      "Batch 400: Train loss: 0.47658426880836485\n",
      "Batch 500: Train loss: 0.47702648490667343\n",
      "Batch 600: Train loss: 0.4565696716308594\n",
      "Batch 700: Train loss: 0.4628025224804878\n",
      "Batch 800: Train loss: 0.4566659690439701\n",
      "Batch 900: Train loss: 0.441707221865654\n",
      "Val loss: 0.47118215462204754\n",
      "Epoch 6\n",
      "Batch 100: Train loss: 0.45488725543022157\n",
      "Batch 200: Train loss: 0.43089222148060796\n",
      "Batch 300: Train loss: 0.44484776988625524\n",
      "Batch 400: Train loss: 0.44417577251791956\n",
      "Batch 500: Train loss: 0.4421414721012116\n",
      "Batch 600: Train loss: 0.4346304176747799\n",
      "Batch 700: Train loss: 0.41542713835835454\n",
      "Batch 800: Train loss: 0.42448060601949694\n",
      "Batch 900: Train loss: 0.41146906957030294\n",
      "Val loss: 0.43933244780370384\n",
      "Epoch 7\n",
      "Batch 100: Train loss: 0.415418002307415\n",
      "Batch 200: Train loss: 0.4194172355532646\n",
      "Batch 300: Train loss: 0.42046513825654985\n",
      "Batch 400: Train loss: 0.39639792501926424\n",
      "Batch 500: Train loss: 0.41364069014787674\n",
      "Batch 600: Train loss: 0.3992287291586399\n",
      "Batch 700: Train loss: 0.4054082228243351\n",
      "Batch 800: Train loss: 0.39246008038520813\n",
      "Batch 900: Train loss: 0.40897422820329665\n",
      "Val loss: 0.4266467055507526\n",
      "Epoch 8\n",
      "Batch 100: Train loss: 0.407780247926712\n",
      "Batch 200: Train loss: 0.3729008060693741\n",
      "Batch 300: Train loss: 0.3952416676282883\n",
      "Batch 400: Train loss: 0.3758336688578129\n",
      "Batch 500: Train loss: 0.37738004118204116\n",
      "Batch 600: Train loss: 0.4006973987817764\n",
      "Batch 700: Train loss: 0.3749809241294861\n",
      "Batch 800: Train loss: 0.3946456816792488\n",
      "Batch 900: Train loss: 0.3935477215051651\n",
      "Val loss: 0.4122269129867007\n",
      "Epoch 9\n",
      "Batch 100: Train loss: 0.3807285086810589\n",
      "Batch 200: Train loss: 0.3780181133747101\n",
      "Batch 300: Train loss: 0.36734679266810416\n",
      "Batch 400: Train loss: 0.37786806270480156\n",
      "Batch 500: Train loss: 0.3900009381771088\n",
      "Batch 600: Train loss: 0.3653367204964161\n",
      "Batch 700: Train loss: 0.36646429091691973\n",
      "Batch 800: Train loss: 0.3676484967768192\n",
      "Batch 900: Train loss: 0.3810085691511631\n",
      "Val loss: 0.3884079536055304\n",
      "Epoch 10\n",
      "Batch 100: Train loss: 0.37806877806782724\n",
      "Batch 200: Train loss: 0.3675193502008915\n",
      "Batch 300: Train loss: 0.3678785994648933\n",
      "Batch 400: Train loss: 0.3653458978235722\n",
      "Batch 500: Train loss: 0.36271428287029267\n",
      "Batch 600: Train loss: 0.3469360263645649\n",
      "Batch 700: Train loss: 0.3600235085189343\n",
      "Batch 800: Train loss: 0.3531798505783081\n",
      "Batch 900: Train loss: 0.3613181219995022\n",
      "Val loss: 0.3830121098809941\n",
      "Epoch 11\n",
      "Batch 100: Train loss: 0.3673555263876915\n",
      "Batch 200: Train loss: 0.3533801704645157\n",
      "Batch 300: Train loss: 0.35038541749119756\n",
      "Batch 400: Train loss: 0.3499109657108784\n",
      "Batch 500: Train loss: 0.3639261718094349\n",
      "Batch 600: Train loss: 0.3333266671001911\n",
      "Batch 700: Train loss: 0.34578871965408325\n",
      "Batch 800: Train loss: 0.3530088958144188\n",
      "Batch 900: Train loss: 0.3585007528960705\n",
      "Val loss: 0.37883862663226525\n",
      "Epoch 12\n",
      "Batch 100: Train loss: 0.3538537845015526\n",
      "Batch 200: Train loss: 0.3427997235953808\n",
      "Batch 300: Train loss: 0.3513432215899229\n",
      "Batch 400: Train loss: 0.35153794020414353\n",
      "Batch 500: Train loss: 0.33246690705418586\n",
      "Batch 600: Train loss: 0.3266449502110481\n",
      "Batch 700: Train loss: 0.3443299672007561\n",
      "Batch 800: Train loss: 0.3402915266156197\n",
      "Batch 900: Train loss: 0.35292568549513814\n",
      "Val loss: 0.38137181834050804\n",
      "Epoch 13\n",
      "Batch 100: Train loss: 0.34751939952373506\n",
      "Batch 200: Train loss: 0.34695444121956825\n",
      "Batch 300: Train loss: 0.33436846151947974\n",
      "Batch 400: Train loss: 0.32871422030031683\n",
      "Batch 500: Train loss: 0.32774842910468577\n",
      "Batch 600: Train loss: 0.33461743846535685\n",
      "Batch 700: Train loss: 0.3478390660881996\n",
      "Batch 800: Train loss: 0.3267547263205051\n",
      "Batch 900: Train loss: 0.32658332243561744\n",
      "Val loss: 0.36725316277355147\n",
      "Epoch 14\n",
      "Batch 100: Train loss: 0.3386098310351372\n",
      "Batch 200: Train loss: 0.3311552286148071\n",
      "Batch 300: Train loss: 0.3357735918462276\n",
      "Batch 400: Train loss: 0.32731289565563204\n",
      "Batch 500: Train loss: 0.31430801570415495\n",
      "Batch 600: Train loss: 0.3309084539860487\n",
      "Batch 700: Train loss: 0.3317235478758812\n",
      "Batch 800: Train loss: 0.3278843040764332\n",
      "Batch 900: Train loss: 0.324993007928133\n",
      "Val loss: 0.3563978764092087\n",
      "Epoch 15\n",
      "Batch 100: Train loss: 0.32485166400671006\n",
      "Batch 200: Train loss: 0.32049077972769735\n",
      "Batch 300: Train loss: 0.3143031679093838\n",
      "Batch 400: Train loss: 0.3284100389480591\n",
      "Batch 500: Train loss: 0.32757276862859724\n",
      "Batch 600: Train loss: 0.33510082349181175\n",
      "Batch 700: Train loss: 0.32751355968415735\n",
      "Batch 800: Train loss: 0.31553491704165937\n",
      "Batch 900: Train loss: 0.32348913490772246\n",
      "Val loss: 0.36825800729784997\n",
      "Epoch 16\n",
      "Batch 100: Train loss: 0.32523870453238485\n",
      "Batch 200: Train loss: 0.3321707692742348\n",
      "Batch 300: Train loss: 0.32309068270027635\n",
      "Batch 400: Train loss: 0.32452881246805193\n",
      "Batch 500: Train loss: 0.3194787700474262\n",
      "Batch 600: Train loss: 0.30764192312955857\n",
      "Batch 700: Train loss: 0.3076238687336445\n",
      "Batch 800: Train loss: 0.3140998914092779\n",
      "Batch 900: Train loss: 0.31647429391741755\n",
      "Val loss: 0.3538853300225203\n",
      "Epoch 17\n",
      "Batch 100: Train loss: 0.3099070668965578\n",
      "Batch 200: Train loss: 0.31275527030229566\n",
      "Batch 300: Train loss: 0.31442554712295534\n",
      "Batch 400: Train loss: 0.31034090064466\n",
      "Batch 500: Train loss: 0.3194611536711454\n",
      "Batch 600: Train loss: 0.312834645062685\n",
      "Batch 700: Train loss: 0.32368359543383124\n",
      "Batch 800: Train loss: 0.3073781515657902\n",
      "Batch 900: Train loss: 0.298055080473423\n",
      "Val loss: 0.3476653267054041\n",
      "Epoch 18\n",
      "Batch 100: Train loss: 0.3268097859621048\n",
      "Batch 200: Train loss: 0.3063255888223648\n",
      "Batch 300: Train loss: 0.3153784066438675\n",
      "Batch 400: Train loss: 0.28987338945269586\n",
      "Batch 500: Train loss: 0.31573654010891916\n",
      "Batch 600: Train loss: 0.30985970981419086\n",
      "Batch 700: Train loss: 0.3066669696569443\n",
      "Batch 800: Train loss: 0.3133841323852539\n",
      "Batch 900: Train loss: 0.29166412219405174\n",
      "Val loss: 0.3453921128989785\n",
      "Epoch 19\n",
      "Batch 100: Train loss: 0.30788172021508214\n",
      "Batch 200: Train loss: 0.31814819931983945\n",
      "Batch 300: Train loss: 0.30175586134195326\n",
      "Batch 400: Train loss: 0.31301598384976387\n",
      "Batch 500: Train loss: 0.3000642216205597\n",
      "Batch 600: Train loss: 0.2879632317274809\n",
      "Batch 700: Train loss: 0.288805041089654\n",
      "Batch 800: Train loss: 0.32572834700345993\n",
      "Batch 900: Train loss: 0.29199278190732003\n",
      "Val loss: 0.3449816974295173\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss_value = loss(outputs, labels)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss_value.item()\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            last_loss = train_loss / 100\n",
    "            print(f\"Batch {i}: Train loss: {last_loss}\")\n",
    "            train_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss_value = loss(outputs, labels)\n",
    "            val_loss += loss_value.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Val loss: {val_loss}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"fashion_mnist_quant.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec824f8a5a7a7587",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:27:13.278896Z",
     "start_time": "2024-07-27T15:27:13.263897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"fashion_mnist_quant.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9ecafb87dcb543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:30:53.933630Z",
     "start_time": "2024-07-27T15:30:53.555184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "Serving 'fashion_mnist_quant.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x73954f3c3fa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "input_shape = (1, 1, 28, 28)\n",
    "inp = torch.rand(input_shape)\n",
    "print(next(val_loader.__iter__())[0].shape)\n",
    "\n",
    "model.cpu()\n",
    "\n",
    "qonnx_path = \"fashion_mnist_quant.onnx\"\n",
    "export_qonnx(model, inp, export_path=qonnx_path)\n",
    "qonnx_cleanup(qonnx_path, out_file=qonnx_path)\n",
    "\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(qonnx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa268e9d9113232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wojciech/finn-examples/build/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "finn_path = \"fashion_mnist_finn.onnx\"\n",
    "\n",
    "model_for_finn = ModelWrapper(qonnx_path)\n",
    "model_for_finn = model_for_finn.transform(ConvertQONNXtoFINN())\n",
    "model_for_finn.save(finn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d5fe56-5171-4d09-9164-edf5571bde96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'fashion_mnist_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x739544b19420>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0307957-4271-4c39-b0ce-67ae0726dbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN output: [[-2.574308  -1.9530442 -6.135125  -1.4271387 -3.5321455  3.7578356\n",
      "  -2.9011254  4.6645055  0.9300549 10.501408 ]]\n",
      "Brevitas output: tensor([[-2.5743, -1.9530, -6.1351, -1.4271, -3.5321,  3.7578, -2.9011,  4.6645,\n",
      "          0.9301, 10.5014]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "model_for_finn = ModelWrapper(finn_path)\n",
    "input_name = model_for_finn.graph.input[0].name\n",
    "input_shape = model_for_finn.get_tensor_shape(input_name)\n",
    "output_name = model_for_finn.graph.output[0].name\n",
    "inp = next(val_loader.__iter__())[0][0:1]\n",
    "inp_dict = {input_name: inp.detach().numpy()}\n",
    "out_dict = oxe.execute_onnx(model_for_finn, inp_dict)\n",
    "\n",
    "print(f\"FINN output: {out_dict[output_name]}\")\n",
    "print(f\"Brevitas output: {model(inp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a71ce3-b888-4e48-b5c7-3e4f4b63342c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
