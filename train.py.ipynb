{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:10.542186Z",
     "start_time": "2024-07-27T15:21:10.539503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: setuptools==69.5.0 in /tmp/home_dir/.local/lib/python3.10/site-packages (69.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools==69.5.0\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat, Int32Bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e030abee64f24243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:11.352482Z",
     "start_time": "2024-07-27T15:21:11.346618Z"
    }
   },
   "outputs": [],
   "source": [
    "WEIGHT_BIT_WIDTH = 8\n",
    "ACT_BIT_WIDTH = 3\n",
    "\n",
    "class QuantModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_0 = qnn.QuantConv2d(\n",
    "            1,\n",
    "            6,\n",
    "            kernel_size=3,\n",
    "            bias=False,\n",
    "            weight_bit_width=WEIGHT_BIT_WIDTH)\n",
    "        self.relu_0 = qnn.QuantReLU(bit_width=ACT_BIT_WIDTH)\n",
    "        self.conv_1 = qnn.QuantConv2d(6, 16, 6,\n",
    "                                      weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "                                      bias=False)\n",
    "        self.relu_1 = qnn.QuantReLU(bit_width=ACT_BIT_WIDTH)\n",
    "        self.conv_2 = qnn.QuantConv2d(16, 128, 4,\n",
    "                                      weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "                                      bias=False)\n",
    "        self.fc1 = qnn.QuantLinear(128, 84,\n",
    "                                   weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "                                   bias=True)\n",
    "        self.relu_2 = qnn.QuantReLU(bit_width=ACT_BIT_WIDTH)\n",
    "        self.fc2 = qnn.QuantLinear(84, 10,\n",
    "                                   weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "                                   bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_0(x)\n",
    "        x = self.relu_0(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2, 2)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2, 2)\n",
    "        x = self.conv_2(x)\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f1fb149c27aef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:12.137494Z",
     "start_time": "2024-07-27T15:21:12.134802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895c6d91687a802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:13.396027Z",
     "start_time": "2024-07-27T15:21:13.392950Z"
    }
   },
   "outputs": [],
   "source": [
    "input_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88205ae5d68494ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:14.254806Z",
     "start_time": "2024-07-27T15:21:14.156238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  60000\n",
      "Val dataset size:  10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST('./data', train=True, download=True, transform=input_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.FashionMNIST('./data', train=False, download=True, transform=input_transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "print(\"Train dataset size: \", len(train_dataset))\n",
    "print(\"Val dataset size: \", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8075071867491929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:21:15.114310Z",
     "start_time": "2024-07-27T15:21:15.086475Z"
    }
   },
   "outputs": [],
   "source": [
    "model = QuantModel().to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f17c9e4cc6dd0bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:26:26.300125Z",
     "start_time": "2024-07-27T15:21:16.385219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Train loss: 2.309877800941467\n",
      "Batch 200: Train loss: 2.236604051589966\n",
      "Batch 300: Train loss: 2.057586097717285\n",
      "Batch 400: Train loss: 1.617937502861023\n",
      "Batch 500: Train loss: 1.30959286570549\n",
      "Batch 600: Train loss: 1.113057897090912\n",
      "Batch 700: Train loss: 1.0147151958942413\n",
      "Batch 800: Train loss: 0.9384061247110367\n",
      "Batch 900: Train loss: 0.8605263912677765\n",
      "Val loss: 0.827880182463652\n",
      "Epoch 1\n",
      "Batch 100: Train loss: 0.801836873292923\n",
      "Batch 200: Train loss: 0.7603584504127503\n",
      "Batch 300: Train loss: 0.7437887847423553\n",
      "Batch 400: Train loss: 0.7246499216556549\n",
      "Batch 500: Train loss: 0.6847815686464309\n",
      "Batch 600: Train loss: 0.6992537569999695\n",
      "Batch 700: Train loss: 0.659537806212902\n",
      "Batch 800: Train loss: 0.650908077955246\n",
      "Batch 900: Train loss: 0.6508463153243065\n",
      "Val loss: 0.6473856923306823\n",
      "Epoch 2\n",
      "Batch 100: Train loss: 0.625213440656662\n",
      "Batch 200: Train loss: 0.6157177484035492\n",
      "Batch 300: Train loss: 0.6062486585974693\n",
      "Batch 400: Train loss: 0.5901076719164848\n",
      "Batch 500: Train loss: 0.5861680191755295\n",
      "Batch 600: Train loss: 0.5864870491623878\n",
      "Batch 700: Train loss: 0.5759626245498657\n",
      "Batch 800: Train loss: 0.5635745891928673\n",
      "Batch 900: Train loss: 0.5731242290139198\n",
      "Val loss: 0.5802178853636335\n",
      "Epoch 3\n",
      "Batch 100: Train loss: 0.5824446815252304\n",
      "Batch 200: Train loss: 0.5453978407382966\n",
      "Batch 300: Train loss: 0.5705442160367966\n",
      "Batch 400: Train loss: 0.5356409722566604\n",
      "Batch 500: Train loss: 0.5262120831012725\n",
      "Batch 600: Train loss: 0.5080027163028717\n",
      "Batch 700: Train loss: 0.5043504032492637\n",
      "Batch 800: Train loss: 0.5028193762898445\n",
      "Batch 900: Train loss: 0.5064105728268623\n",
      "Val loss: 0.534546505493723\n",
      "Epoch 4\n",
      "Batch 100: Train loss: 0.5140345677733421\n",
      "Batch 200: Train loss: 0.5008697110414505\n",
      "Batch 300: Train loss: 0.50746821641922\n",
      "Batch 400: Train loss: 0.49454350501298905\n",
      "Batch 500: Train loss: 0.4843833735585213\n",
      "Batch 600: Train loss: 0.47912377774715426\n",
      "Batch 700: Train loss: 0.47397097885608674\n",
      "Batch 800: Train loss: 0.4879912126064301\n",
      "Batch 900: Train loss: 0.46088683187961577\n",
      "Val loss: 0.4907636084374349\n",
      "Epoch 5\n",
      "Batch 100: Train loss: 0.4628590914607048\n",
      "Batch 200: Train loss: 0.4651821985840797\n",
      "Batch 300: Train loss: 0.47092521235346796\n",
      "Batch 400: Train loss: 0.46102608382701876\n",
      "Batch 500: Train loss: 0.47158881902694705\n",
      "Batch 600: Train loss: 0.45366668060421944\n",
      "Batch 700: Train loss: 0.4452984094619751\n",
      "Batch 800: Train loss: 0.44868768364191053\n",
      "Batch 900: Train loss: 0.44356785118579867\n",
      "Val loss: 0.4570289197241425\n",
      "Epoch 6\n",
      "Batch 100: Train loss: 0.44313531965017317\n",
      "Batch 200: Train loss: 0.45217826306819914\n",
      "Batch 300: Train loss: 0.4346104347705841\n",
      "Batch 400: Train loss: 0.42405113369226455\n",
      "Batch 500: Train loss: 0.43627374842762945\n",
      "Batch 600: Train loss: 0.4393537327647209\n",
      "Batch 700: Train loss: 0.44345163732767107\n",
      "Batch 800: Train loss: 0.4238100571930408\n",
      "Batch 900: Train loss: 0.4153506124019623\n",
      "Val loss: 0.4322490215681161\n",
      "Epoch 7\n",
      "Batch 100: Train loss: 0.4328612077236176\n",
      "Batch 200: Train loss: 0.4097848907113075\n",
      "Batch 300: Train loss: 0.4156304767727852\n",
      "Batch 400: Train loss: 0.4103607778251171\n",
      "Batch 500: Train loss: 0.42567708402872084\n",
      "Batch 600: Train loss: 0.4293327857553959\n",
      "Batch 700: Train loss: 0.39643189683556557\n",
      "Batch 800: Train loss: 0.4233541022241116\n",
      "Batch 900: Train loss: 0.39672536849975587\n",
      "Val loss: 0.43305452898809105\n",
      "Epoch 8\n",
      "Batch 100: Train loss: 0.41286479085683825\n",
      "Batch 200: Train loss: 0.4173985821008682\n",
      "Batch 300: Train loss: 0.388426520973444\n",
      "Batch 400: Train loss: 0.41126009419560433\n",
      "Batch 500: Train loss: 0.39679058954119684\n",
      "Batch 600: Train loss: 0.39168338745832443\n",
      "Batch 700: Train loss: 0.4012775382399559\n",
      "Batch 800: Train loss: 0.3757658350467682\n",
      "Batch 900: Train loss: 0.39143433079123496\n",
      "Val loss: 0.41577419353898165\n",
      "Epoch 9\n",
      "Batch 100: Train loss: 0.39374644741415976\n",
      "Batch 200: Train loss: 0.38670111998915674\n",
      "Batch 300: Train loss: 0.39126461297273635\n",
      "Batch 400: Train loss: 0.3937592074275017\n",
      "Batch 500: Train loss: 0.3932624420523643\n",
      "Batch 600: Train loss: 0.3819589628279209\n",
      "Batch 700: Train loss: 0.3856474132835865\n",
      "Batch 800: Train loss: 0.36549204096198085\n",
      "Batch 900: Train loss: 0.38656108304858205\n",
      "Val loss: 0.3941818755713238\n",
      "Epoch 10\n",
      "Batch 100: Train loss: 0.39742590084671975\n",
      "Batch 200: Train loss: 0.36474411249160765\n",
      "Batch 300: Train loss: 0.38570372834801675\n",
      "Batch 400: Train loss: 0.3526450471580029\n",
      "Batch 500: Train loss: 0.3888234946131706\n",
      "Batch 600: Train loss: 0.36938517093658446\n",
      "Batch 700: Train loss: 0.3852269423007965\n",
      "Batch 800: Train loss: 0.36328811645507814\n",
      "Batch 900: Train loss: 0.3592871867120266\n",
      "Val loss: 0.3955522870561879\n",
      "Epoch 11\n",
      "Batch 100: Train loss: 0.38070038065314293\n",
      "Batch 200: Train loss: 0.36643026664853096\n",
      "Batch 300: Train loss: 0.3596214275062084\n",
      "Batch 400: Train loss: 0.3711744187772274\n",
      "Batch 500: Train loss: 0.3688220266997814\n",
      "Batch 600: Train loss: 0.3548208014667034\n",
      "Batch 700: Train loss: 0.36300427466630936\n",
      "Batch 800: Train loss: 0.3547442391514778\n",
      "Batch 900: Train loss: 0.36186955258250236\n",
      "Val loss: 0.37620162793025846\n",
      "Epoch 12\n",
      "Batch 100: Train loss: 0.3624519966542721\n",
      "Batch 200: Train loss: 0.3545827142894268\n",
      "Batch 300: Train loss: 0.3416463415324688\n",
      "Batch 400: Train loss: 0.3852911379933357\n",
      "Batch 500: Train loss: 0.3672845706343651\n",
      "Batch 600: Train loss: 0.37426294200122356\n",
      "Batch 700: Train loss: 0.3404756818711758\n",
      "Batch 800: Train loss: 0.35131881818175315\n",
      "Batch 900: Train loss: 0.33255248695611955\n",
      "Val loss: 0.3806598703762528\n",
      "Epoch 13\n",
      "Batch 100: Train loss: 0.35221246778965\n",
      "Batch 200: Train loss: 0.3655351151525974\n",
      "Batch 300: Train loss: 0.35743583291769027\n",
      "Batch 400: Train loss: 0.35624058336019515\n",
      "Batch 500: Train loss: 0.34678024381399153\n",
      "Batch 600: Train loss: 0.3448742105066776\n",
      "Batch 700: Train loss: 0.3513015477359295\n",
      "Batch 800: Train loss: 0.32003062531352044\n",
      "Batch 900: Train loss: 0.341119879335165\n",
      "Val loss: 0.3650845590100926\n",
      "Epoch 14\n",
      "Batch 100: Train loss: 0.355556757748127\n",
      "Batch 200: Train loss: 0.36012106135487554\n",
      "Batch 300: Train loss: 0.3427688570320606\n",
      "Batch 400: Train loss: 0.3273980724066496\n",
      "Batch 500: Train loss: 0.34970517084002495\n",
      "Batch 600: Train loss: 0.33732320040464403\n",
      "Batch 700: Train loss: 0.3184694023430347\n",
      "Batch 800: Train loss: 0.33908332243561745\n",
      "Batch 900: Train loss: 0.3372006869316101\n",
      "Val loss: 0.35759501320541287\n",
      "Epoch 15\n",
      "Batch 100: Train loss: 0.358425617814064\n",
      "Batch 200: Train loss: 0.341210123449564\n",
      "Batch 300: Train loss: 0.3283653321862221\n",
      "Batch 400: Train loss: 0.33918935552239415\n",
      "Batch 500: Train loss: 0.32157550767064097\n",
      "Batch 600: Train loss: 0.3385994590818882\n",
      "Batch 700: Train loss: 0.32369970858097075\n",
      "Batch 800: Train loss: 0.343275374174118\n",
      "Batch 900: Train loss: 0.33829975545406343\n",
      "Val loss: 0.35612858774935363\n",
      "Epoch 16\n",
      "Batch 100: Train loss: 0.3426739251613617\n",
      "Batch 200: Train loss: 0.3253909882903099\n",
      "Batch 300: Train loss: 0.31621327742934224\n",
      "Batch 400: Train loss: 0.3284749747812748\n",
      "Batch 500: Train loss: 0.3219850850850344\n",
      "Batch 600: Train loss: 0.32707075893878934\n",
      "Batch 700: Train loss: 0.3269376277923584\n",
      "Batch 800: Train loss: 0.33261618077754973\n",
      "Batch 900: Train loss: 0.33842043042182923\n",
      "Val loss: 0.35026887362929665\n",
      "Epoch 17\n",
      "Batch 100: Train loss: 0.32501488298177716\n",
      "Batch 200: Train loss: 0.33310780584812166\n",
      "Batch 300: Train loss: 0.3141822308301926\n",
      "Batch 400: Train loss: 0.32025330893695353\n",
      "Batch 500: Train loss: 0.3262429240345955\n",
      "Batch 600: Train loss: 0.3272325124591589\n",
      "Batch 700: Train loss: 0.33563133522868155\n",
      "Batch 800: Train loss: 0.31115254186093805\n",
      "Batch 900: Train loss: 0.3315811507403851\n",
      "Val loss: 0.3486018233997807\n",
      "Epoch 18\n",
      "Batch 100: Train loss: 0.3227882105112076\n",
      "Batch 200: Train loss: 0.3194145242869854\n",
      "Batch 300: Train loss: 0.32063811883330345\n",
      "Batch 400: Train loss: 0.30668865144252777\n",
      "Batch 500: Train loss: 0.31372845619916917\n",
      "Batch 600: Train loss: 0.3100280170142651\n",
      "Batch 700: Train loss: 0.3221150316298008\n",
      "Batch 800: Train loss: 0.3266600339114666\n",
      "Batch 900: Train loss: 0.31836308509111405\n",
      "Val loss: 0.354370730697729\n",
      "Epoch 19\n",
      "Batch 100: Train loss: 0.3108914239704609\n",
      "Batch 200: Train loss: 0.3110672141611576\n",
      "Batch 300: Train loss: 0.31029908880591395\n",
      "Batch 400: Train loss: 0.3243419810384512\n",
      "Batch 500: Train loss: 0.30655314177274706\n",
      "Batch 600: Train loss: 0.312585037201643\n",
      "Batch 700: Train loss: 0.31762609630823135\n",
      "Batch 800: Train loss: 0.322912664860487\n",
      "Batch 900: Train loss: 0.3122414223849773\n",
      "Val loss: 0.3485066175081168\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss_value = loss(outputs, labels)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss_value.item()\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            last_loss = train_loss / 100\n",
    "            print(f\"Batch {i}: Train loss: {last_loss}\")\n",
    "            train_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss_value = loss(outputs, labels)\n",
    "            val_loss += loss_value.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Val loss: {val_loss}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"fashion_mnist_quant.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec824f8a5a7a7587",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:27:13.278896Z",
     "start_time": "2024-07-27T15:27:13.263897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"fashion_mnist_quant.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9ecafb87dcb543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:30:53.933630Z",
     "start_time": "2024-07-27T15:30:53.555184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "Serving 'fashion_mnist_quant.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x74e4301d0f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "input_shape = (1, 1, 28, 28)\n",
    "inp = torch.rand(input_shape)\n",
    "print(next(val_loader.__iter__())[0].shape)\n",
    "\n",
    "model.cpu()\n",
    "\n",
    "qonnx_path = \"fashion_mnist_quant.onnx\"\n",
    "export_qonnx(model, inp, export_path=qonnx_path)\n",
    "qonnx_cleanup(qonnx_path, out_file=qonnx_path)\n",
    "\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(qonnx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa268e9d9113232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wojciech/finn-examples/build/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "finn_path = \"fashion_mnist_finn.onnx\"\n",
    "\n",
    "model_for_finn = ModelWrapper(qonnx_path)\n",
    "model_for_finn = model_for_finn.transform(ConvertQONNXtoFINN())\n",
    "model_for_finn.save(finn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d5fe56-5171-4d09-9164-edf5571bde96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'fashion_mnist_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x74e440b17d90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0307957-4271-4c39-b0ce-67ae0726dbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 28, 28]\n",
      "FINN output: [[-1.3989073  -9.333837   -3.0418384  -3.5792613  -3.0741603   4.254069\n",
      "  -2.909774    5.8974247   0.96250105  8.780479  ]]\n",
      "Brevitas output: tensor([[-1.3989, -9.3338, -3.0418, -3.5793, -3.0742,  4.2541, -2.9098,  5.8974,\n",
      "          0.9625,  8.7805]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "model_for_finn = ModelWrapper(finn_path)\n",
    "input_name = model_for_finn.graph.input[0].name\n",
    "input_shape = model_for_finn.get_tensor_shape(input_name)\n",
    "output_name = model_for_finn.graph.output[0].name\n",
    "inp = next(val_loader.__iter__())[0][0:1]\n",
    "inp_dict = {input_name: inp.detach().numpy()}\n",
    "out_dict = oxe.execute_onnx(model_for_finn, inp_dict)\n",
    "\n",
    "print(f\"FINN output: {out_dict[output_name]}\")\n",
    "print(f\"Brevitas output: {model(inp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a71ce3-b888-4e48-b5c7-3e4f4b63342c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
